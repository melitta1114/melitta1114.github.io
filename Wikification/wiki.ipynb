{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b05421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c4d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_WIKI = 'enwiki-20061130-pages-articles.xml'\n",
    "WIKI_PAGES = 'wikipedia_pages.csv'\n",
    "DISAMBIGUATION_PAGES = 'disambiguation.csv'\n",
    "ENCODING = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de369e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# egy XML elem nevéből eltávolítja a namespace előtagot\n",
    "def strip_tag_name(t):\n",
    "\n",
    "    # a tag lekérése az elem objektumból\n",
    "    t = elem.tag\n",
    "    \n",
    "    # megkeresi a jobb oldali legutolsó kapcsos zárójelet\n",
    "    idx = t.rfind(\"}\")\n",
    "    \n",
    "    # csak a namespace utáni részt tartja meg\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# megnyitjuk a két CSV fájlt írásra\n",
    "# az egyik fájl a rendes Wikipedia oldalaknak van\n",
    "# a másik a disambiguation oldalaknak\n",
    "with codecs.open(WIKI_PAGES, \"w\", ENCODING) as wikiPage, \\\n",
    "    codecs.open(DISAMBIGUATION_PAGES, \"w\", ENCODING) as disambiguationPage:\n",
    "    \n",
    "    # QUOTE_MINIMAL azt jelenti, hogy csak akkor rak idézőjeleket a mezők köré, ha szükséges\n",
    "    wikiPageWriter = csv.writer(wikiPage, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # az első sorban fejléc: title és text\n",
    "    wikiPageWriter.writerow(['title', 'text'])\n",
    "    \n",
    "    disambiguationPageWriter = csv.writer(disambiguationPage, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    disambiguationPageWriter.writerow(['title', 'text'])\n",
    "    \n",
    "    \n",
    "    # XML fájl feldolgozása eseményalapon (iteratív olvasás, nem egyszerre az egész fájlba memóriába!).\n",
    "    for event, elem in etree.iterparse(FILENAME_WIKI, events=('start', 'end')):\n",
    "        \n",
    "        # megtisztítjuk az elem nevét\n",
    "        tname = strip_tag_name(elem.tag)\n",
    "        \n",
    "        # ha az esemény típusa start (tehát egy elem kezdetén járunk)\n",
    "        if event == 'start':\n",
    "            # ha az elem neve title, akkor eltároljuk a címet\n",
    "            if tname == 'title':\n",
    "                title = elem.text\n",
    "            \n",
    "            # ha az elem neve text, tehát a cikk szövege\n",
    "            elif tname == 'text':\n",
    "                # ellenőrizzük, hogy a szöveg és a cím nem üres\n",
    "                if elem.text != None and title != None and 'disambiguation' in title:\n",
    "                    # ha a cím tartalmazza a disambiguation szót, akkor egyértelműsítő oldalnak tekintjük\n",
    "                    disambiguationPageWriter.writerow([title, elem.text])\n",
    "                \n",
    "      \n",
    "                elif elem.text != None and title != None and len(elem.text) > 1024 and \"Image:\" not in title and 'This page has been deleted' not in elem.text:\n",
    "                    wikiPageWriter.writerow([title, elem.text])\n",
    "        \n",
    "        # felszabadítjuk az aktuális elem memóriáját, hogy ne fogyjon el a memória a nagy XML feldolgozásakor\n",
    "        elem.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee53e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('wikipedia_pages.csv')\n",
    "disambiguation = pd.read_csv('disambiguation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# létrehozunk egy szótárt, amelynek kulcsai a pages DataFrame title oszlopából származó címek kisbetűs változatai\n",
    "# minden kulcshoz az érték 1 lesz\n",
    "\n",
    "keywords_titles = {}\n",
    "for i in range(0, len(pages)):\n",
    "    keywords_titles[str(pages['title'][i]).lower()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_roots = {}\n",
    "\n",
    "\n",
    "for keyword in keywords_titles:\n",
    "    \n",
    "    # a címet szóköz mentén feldaraboljuk szavakra, minden szóra alkalmazzuk a szótövezőt (stemmer),\n",
    "    root = ' '.join(map(str, [stemmer.stem(word) for word in keyword.split(' ')]))\n",
    "\n",
    "    if root != keyword:\n",
    "        titles_roots[root] = keywords_titles[keyword]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861b56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# létrehozunk egy szótárat (dictionary-t), amely a stopwords-kat tartalmazza\n",
    "# minden stop-szót kulcsként tárolunk a szótárban, és az értékét 1-re állítjuk.\n",
    "\n",
    "stopwords_dict = {k: 1 for k in stopwords.words('english')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a2a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a függvény visszaad egy listát, amely az adott szövegből előállított n-grammákat tartalmazza\n",
    "def get_ngrams(filtered_article, n):\n",
    "    \n",
    "    # Minden szó a szöveg egy lehetséges eltolásával kerül párba. Az n elem minden egyes pozícióra generál egy n-grammot\n",
    "    ngrams = zip(*[filtered_article[i:] for i in range(n)])\n",
    "    \n",
    "    # az n-grammák listájának elkészítése\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e71413",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_documents_with_word_as_keyword = {}\n",
    "nr_of_documents_with_word = {}\n",
    "\n",
    "\n",
    "# végigmegyünk az összes dokumentum szövegén (feltételezve, hogy ez egy lista vagy sorozat)\n",
    "for text in pages['text']:\n",
    "        \n",
    "    words_this_text = {}\n",
    "    tokenized_article = word_tokenize(text)\n",
    "\n",
    "    # csak azokat a szavakat tartjuk meg, amelyek betűkből állnak\n",
    "    # és nem szerepelnek a stopwordsben\n",
    "    filtered_article = [w.lower() for w in tokenized_article if w.isalpha() and stopwords_dict.get(w.lower(), 0) == 0]\n",
    "    \n",
    "    # ha a szó szótöve szerepel a címek gyökerei között, vagy kulcsszóként ismert\n",
    "    for word in filtered_article:\n",
    "        # hozzáadjuk a szót a dokumentumban talált releváns szavakhoz\n",
    "        if titles_roots.get(stemmer.stem(word), 0) == 1 or keywords_titles.get(word, 0) == 1:\n",
    "            words_this_text[word] = words_this_text.get(word, 0) + 1\n",
    "    \n",
    "    \n",
    "    for n in range(2, 4):\n",
    "        ngrams = get_ngrams(filtered_article, n)\n",
    "        for ngram in ngrams:\n",
    "            root = ' '.join(map(str, [stemmer.stem(word) for word in ngram.split(' ')]))\n",
    "            # ha ez a szótövek kombinációja vagy maga az ngram szerepel a kulcsszók között\n",
    "            if titles_roots.get(root, 0) == 1 or keywords_titles.get(ngram, 0) == 1:\n",
    "                # hozzáadjuk a dokumentumban talált releváns szókapcsolatokhoz\n",
    "                words_this_text[ngram] = words_this_text.get(ngram, 0) + 1\n",
    "        \n",
    "        # kiválasztjuk a leggyakoribb (felső 6%-nyi) szavakat, mint potenciális kulcsszavakat\n",
    "    keywords_to_consider = sorted(words_this_text.items(), key=lambda kv: kv[1], reverse=True)[:int(len(words_this_text) * 0.06)] \n",
    "    \n",
    "    # növeljük az összes dokumentumban előforduló szavak számlálóját\n",
    "    for word in words_this_text.keys():\n",
    "        nr_of_documents_with_word[word] = nr_of_documents_with_word.get(word, 0) + 1\n",
    "        \n",
    "    # növeljük azoknak a szavaknak a számlálóját, amelyek ebben a dokumentumban kulcsszóként jelentek meg\n",
    "    for word in keywords_to_consider:\n",
    "        nr_of_documents_with_word_as_keyword[word[0]] = nr_of_documents_with_word_as_keyword.get(word[0], 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da81ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nr_of_documents_with_word_as_keyword.json\", \"w\") as file:\n",
    "    json.dump(nr_of_documents_with_word_as_keyword, file, indent=4)\n",
    "\n",
    "with open(\"nr_of_documents_with_word.json\", \"w\") as file:\n",
    "    json.dump(nr_of_documents_with_word , file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_roots_lists = {}\n",
    "\n",
    "for keyword in keywords_titles:\n",
    "    root = ' '.join(map(str, [stemmer.stem(word) for word in keyword.split(' ')]))\n",
    "\n",
    "    # csak akkor dolgozzuk fel, ha a root nem egyezik az eredeti címmel\n",
    "    if root != keyword:\n",
    "        # ha ez a root már létezik és a keyword még nincs a listában, hozzáadjuk\n",
    "        if root in titles_roots_lists:\n",
    "            if keyword not in titles_roots_lists[root]:\n",
    "                titles_roots_lists[root].append(keyword)\n",
    "        else:\n",
    "            # ha még nem szerepel a root, új listával indítjuk\n",
    "            titles_roots_lists[root] = [keyword]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e979dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nr_of_documents_with_word.json') as json_file:\n",
    "    nr_of_documents_with_word = json.load(json_file)\n",
    "\n",
    "with open('nr_of_documents_with_word_as_keyword.json') as json_file:\n",
    "    nr_of_documents_with_word_as_keyword = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2abc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(nr_of_documents_with_word.keys())\n",
    "\n",
    "for key in keys:\n",
    "    # ha egy szó legfeljebb 5 dokumentumban fordul elő, eltávolítjuk\n",
    "    if nr_of_documents_with_word[key] <= 5:\n",
    "        nr_of_documents_with_word.pop(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89add057",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(nr_of_documents_with_word_as_keyword.keys())\n",
    "\n",
    "for key in keys:\n",
    "    if nr_of_documents_with_word_as_keyword[key] <= 5:\n",
    "        nr_of_documents_with_word_as_keyword.pop(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf657d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# szótár létrehozása, amelyben a dokumentumokban szereplő szavak rootjai vannak eltárolva\n",
    "# a kulcs: root, az érték: annak maximális előfordulása dokumentumkulcsszóként\n",
    "nr_of_documents_with_word_roots = {}\n",
    "\n",
    "# végigmegyünk az összes szón (vagy n-gramon)\n",
    "for keyword in nr_of_documents_with_word:\n",
    "\n",
    "    # az adott szó(n-gram) szótövét képezzük úgy, hogy minden szót külön stemmelünk, majd visszarakjuk őket egy sztringgé\n",
    "    root = ' '.join(map(str, [stemmer.stem(word) for word in keyword.split(' ')]))\n",
    "\n",
    "    # csak akkor folytatjuk, ha a szótő különbözik az eredeti szótól\n",
    "    if root != keyword:\n",
    "        # a szótőhöz tartozó érték a korábban eltárolt érték és az aktuális keyword-hoz tartozó érték maximuma lesz\n",
    "        # így mindig a legnagyobb dokumentumszámot tartjuk meg, ahol a root előfordul\n",
    "        nr_of_documents_with_word_roots[root] = max(\n",
    "            nr_of_documents_with_word_roots.get(root, 0),  # ha már szerepel, lekérjük az eddigi értéket\n",
    "            nr_of_documents_with_word[keyword]             # összevetjük a jelenlegi kulcsszó előfordulásával\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac69fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_documents_with_word_as_keyword_roots = {}\n",
    "\n",
    "for keyword in nr_of_documents_with_word_as_keyword:\n",
    "    root = ' '.join(map(str, [stemmer.stem(word) for word in keyword.split(' ')]))\n",
    "    if root != keyword:\n",
    "        nr_of_documents_with_word_as_keyword_roots[root] = max(nr_of_documents_with_word_as_keyword_roots.get(root, 0), nr_of_documents_with_word_as_keyword[keyword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab0738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ez a függvény visszaadja, hogy egy adott szó (vagy kifejezés) hány dokumentumban szerepelt\n",
    "# először pontos egyezést keres, ha nincs, akkor a szótövére próbál keresni\n",
    "\n",
    "def n(w):\n",
    "    # megnézzük, hogy a szó (w) pontosan hány dokumentumban szerepel\n",
    "    nr = nr_of_documents_with_word.get(w, 0)\n",
    "    if nr != 0:\n",
    "        return nr\n",
    "\n",
    "    # ha a pontos szó nem szerepel, megnézzük a szótövéhez tartozó dokumentumszámot\n",
    "    nr = nr_of_documents_with_word_roots.get(w, 0)\n",
    "    if nr != 0:\n",
    "        return nr\n",
    "    \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db67837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a függvény célja, hogy visszaadja a szövegben található szavak és n-gramok előfordulásait\n",
    "# szavanként számolja meg, hogy hányszor fordulnak elő, valamint a 2-gramokat és 3-gramokat is\n",
    "def get_occurences_of_word(text):    \n",
    "    # szótár, amely a szavakat és azok előfordulási számát tartalmazza a szövegben\n",
    "    words_this_text = {}\n",
    "    \n",
    "    # a szöveget tokenizáljuk (szavakra bontjuk), majd kisbetűssé alakítjuk\n",
    "    # csak azokat a szavakat tartjuk meg, amelyek betűkből állnak és nem szerepelnek a stop szavak között\n",
    "    filtered_article = [w.lower() for w in word_tokenize(text) if w.isalpha() and stopwords_dict.get(w.lower(), 0) == 0]\n",
    "    \n",
    "    for word in filtered_article:\n",
    "        # ha a szó már szerepel a szótárban, növeljük a számlálót, ha nem, akkor 1-re állítjuk\n",
    "        words_this_text[word] = words_this_text.get(word, 0) + 1\n",
    "    \n",
    "    for n in range(2, 4):\n",
    "        # n-gramokat generálunk a szövegből (2-gramok és 3-gramok)\n",
    "        ngrams = get_ngrams(filtered_article, n)\n",
    "        for ngram in ngrams:\n",
    "            # A generált n-gramokat is hozzáadjuk a szótárhoz, és növeljük azok előfordulását\n",
    "            words_this_text[ngram] = words_this_text.get(ngram, 0) + 1\n",
    "    \n",
    "    return words_this_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2b1be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a szavak előfordulásainak értékei alapján kiválassza a linkelendő kulcsszavakat\n",
    "def words_to_link(scores):\n",
    "    # lekérjük azokat a kulcsszavakat, amelyek az előfordulások szerint a legfontosabbak\n",
    "    # a 'scores' egy szótár, ahol a kulcsok a szavak, az értékek pedig az előfordulásuk számát jelzik\n",
    "    # az első 6%-át vesszük a legnagyobb előfordulási számmal rendelkező szavaknak\n",
    "    keywords_to_consider = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:int(len(scores) * 0.06)]\n",
    "    \n",
    "    link = []\n",
    "\n",
    "    for ngram in [item[0] for item in keywords_to_consider]:\n",
    "        # a kulcsszót (ngram) szótövezés alá vetjük, hogy egységesebbek legyenek a formák\n",
    "        root = ' '.join(map(str, [stemmer.stem(word) for word in ngram.split(' ')]))\n",
    "        \n",
    "        # ha a szótő vagy a kulcsszó szerepel a címekhez rendelt szótárban, akkor ezt hozzáadjuk a linkekhez\n",
    "        if titles_roots_lists.get(root, 0) != 0 or keywords_titles.get(ngram, 0) != 0:\n",
    "            link.append(ngram)\n",
    "            \n",
    "    # töröljük a redundáns elemeket (pl. ha egy n-gram több szót tartalmaz, és az egyes szavak is szerepelnek a listában)\n",
    "    for item in link:\n",
    "        # csak azokat az n-gramokat nézzük, amelyek több mint egy szóból állnak\n",
    "        if len(item.split(' ')) >= 2:\n",
    "            for word in item.split(' '):\n",
    "                # ha a szó szerepel az n-gramok között, akkor eltávolítjuk\n",
    "                if word in link:\n",
    "                    link.remove(word)\n",
    "        \n",
    "    return link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38172a3",
   "metadata": {},
   "source": [
    "Tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7062b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tf kiszámolja egy szó gyakoriságát egy adott szövegben\n",
    "# 'w' a szó, amit keresünk, words pedig egy szótár, amely tartalmazza a szó előfordulásait\n",
    "def tf(w, words):\n",
    "    # a szó előfordulása a szövegben: ha a szó szerepel, visszaadjuk az előfordulás számát,\n",
    "    # ha nem szerepel, akkor 0-t adunk vissza.\n",
    "    return words.get(w, 0)\n",
    "\n",
    "# a tf-idf (Term Frequency-Inverse Document Frequency) függvény célja, hogy kiszámolja\n",
    "# egy szó súlyát, figyelembe véve a szó előfordulását egy szövegben (tf) és annak ritkaságát\n",
    "# az összes dokumentumban (idf)\n",
    "def tf_idf(w, words):\n",
    "    # 'words' egy szótár, amely a szövegben található szavakat és azok előfordulásait tartalmazza\n",
    "    # az 'n(w)' a szó előfordulásának számát adja vissza az összes dokumentumban\n",
    "    nr = n(w)\n",
    "    \n",
    "    # Ha a szó előfordul legalább egyszer (nr != 0), akkor kiszámoljuk a tf-idf értékét\n",
    "    if nr != 0:\n",
    "        # a tf-idf értéke: tf(w) * log(N / n(w)), ahol N az összes dokumentumok száma\n",
    "        # n(w) pedig a szó előfordulásainak száma\n",
    "        return tf(w, words) * math.log(N / n(w))\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f826d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meghatározza a szövegben található kulcsszavakat a tf-idf alapján, \n",
    "# majd kiválassza azokat a szavakat, amelyek linkelhetők.\n",
    "def get_words_to_link_tf_idf(text):\n",
    "    # először is meghatározzuk a szó előfordulásokat a szövegben\n",
    "    # a get_occurences_of_word függvény egy szótárat ad vissza, amely tartalmazza a szövegben\n",
    "    # található szavakat és azok előfordulásait\n",
    "    words_in_this_text = get_occurences_of_word(text)\n",
    "    \n",
    "    # kiszámítjuk a tf-idf értékeket a szavakra\n",
    "    # A tf-idf_w egy szótár, amely tartalmazza a szavakat és azok tf-idf értékeit\n",
    "    tf_idf_w = {}\n",
    "\n",
    "    for w in words_in_this_text.keys():\n",
    "        # a tf-idf érték kiszámolása a szó számára a get_occurences_of_word által visszaadott szótár\n",
    "        # és a tf_idf függvény használatával\n",
    "        tf_idf_w[w] = tf_idf(w, words_in_this_text)\n",
    "    \n",
    "    # visszaadjuk a legfontosabb kulcsszavakat\n",
    "    # a words_to_link függvény fogja meghatározni, hogy mely szavakat kell linkelni\n",
    "    return words_to_link(tf_idf_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c85d02",
   "metadata": {},
   "source": [
    "Keyphraseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha a szó nem szerepel közvetlenül, akkor a szótövének előfordulásait is figyelembe veszi\n",
    "def n_keyword(w):\n",
    "    # ellenőrizzük, hogy a szó hány dokumentumban szerepel kulcsszóként\n",
    "    # ha a szó szerepel a kulcsszavak között, akkor visszaadjuk az előfordulás számát\n",
    "    nr = nr_of_documents_with_word_as_keyword.get(w, 0)\n",
    "    if nr != 0:\n",
    "        return nr\n",
    "    \n",
    "    # ha a szó nem szerepel közvetlenül, akkor megnézzük a szótövének előfordulását\n",
    "    # hogy a szótöve szerepel-e kulcsszóként más dokumentumokban\n",
    "    nr = nr_of_documents_with_word_as_keyword_roots.get(w, 0)\n",
    "    if nr != 0:\n",
    "        return nr\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "869f25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a kulcsszóra vonatkozó arányt adja vissza, azaz a kulcsszó gyakoriságát a szó összes előfordulásához képest\n",
    "def keyphraseness_score(w):\n",
    "    # ha a szó előfordulásának száma nem 0\n",
    "    # akkor kiszámítjuk a kulcsszóra vonatkozó arányt.\n",
    "    if n(w) != 0:\n",
    "        # a kulcsszóhoz tartozó előfordulások számát elosztjuk a szó összes előfordulásával.\n",
    "        # ez adja a kulcsszóra vonatkozó arányt\n",
    "        return n_keyword(w) / n(w)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84edd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a legfontosabb kulcsszavakat határozza meg, amelyeket linkelni érdemes.\n",
    "def get_words_to_link_keyphraseness(text):\n",
    "    # először kellenek a szövegben előforduló szavakat és azok előfordulásai\n",
    "    words_in_this_text = get_occurences_of_word(text)\n",
    "    \n",
    "    # Létrehozzuk a kulcsszóra vonatkozó értékeléseket\n",
    "    keyphraseness_scores = {}\n",
    "\n",
    "    # minden szóra kiszámítjuk a kulcsszó értékelést\n",
    "    for w in words_in_this_text.keys():\n",
    "        keyphraseness_scores[w] = keyphraseness_score(w)\n",
    "    \n",
    "    # a kiszámított kulcsszó értékelések alapján meghatározzuk a linkelhető szavakat\n",
    "    return words_to_link(keyphraseness_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d49b83",
   "metadata": {},
   "source": [
    "Összehasonlítás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b78432c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Machine learning is a branch of artificial intelligence that focuses on building systems that can learn from data. These systems improve their performance over time without being\" \\\n",
    "\"explicitly programmed. Applications include image recognition, natural language processing, and recommendation systems. It is widely used in industries such as healthcare, finance\" \\\n",
    "\"and marketing.\"\n",
    "\n",
    "\n",
    "keyphraseness_words = get_words_to_link_keyphraseness(text)\n",
    "\n",
    "tf_idf_words = get_words_to_link_tf_idf(text)\n",
    "\n",
    "\n",
    "# közös kulcsszavak keresése a két lista között\n",
    "common_keywords = set(keyphraseness_words).intersection(tf_idf_words)\n",
    "\n",
    "# közös kulcsszavak számossága\n",
    "common_count = len(common_keywords)\n",
    "\n",
    "# kulcsszavak száma mindkét listában\n",
    "keyphraseness_count = len(set(keyphraseness_words))\n",
    "tf_idf_count = len(set(tf_idf_words))\n",
    "\n",
    "# arány számítása\n",
    "common_ratio_keyphraseness = common_count / keyphraseness_count if keyphraseness_count != 0 else 0\n",
    "common_ratio_tf_idf = common_count / tf_idf_count if tf_idf_count != 0 else 0\n",
    "\n",
    "# a közös kulcsszavak aránya\n",
    "common_ratio = common_count / max(keyphraseness_count, tf_idf_count) if max(keyphraseness_count, tf_idf_count) != 0 else 0\n",
    "\n",
    "common_ratio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
